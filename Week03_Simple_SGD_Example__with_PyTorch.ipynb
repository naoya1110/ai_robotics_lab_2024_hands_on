{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdFZvUV3I2FBWzEds6G8iE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naoya1110/ai_robotics_lab_2024_hands_on/blob/main/Week03_Simple_SGD_Example__with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3N0fVQqIH-w"
      },
      "source": [
        "This is supplemental material for Week 03, explaining how model parameters are optimized using the stochastic gradient descent (SGD) method.\n",
        "\n",
        "In this example, we will implement a simple linear regression model with PyTorch.\n",
        "\n",
        "First, let's import the PyTorch, NumPy, and Matplotlib packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVx3kr0D4Ppr"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFGAxPumJKhm"
      },
      "source": [
        "Here, we create a dataset consisting of $x$ (inputs) and $y$ (outputs) using a simple linear equation below. It's important to note that the output data y contains some random noise.\n",
        "\n",
        "$y = 5x + 3 + \\mathrm{noise}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d12jiZah4lAV"
      },
      "source": [
        "x = 10*np.random.rand(100)-5\n",
        "noise = 3*np.random.randn(x.shape[0])\n",
        "y = 5*x + 3 + noise\n",
        "\n",
        "plt.plot(x, y, marker=\"o\", lw= 0, label=\"data\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1d9TcZJKOAH"
      },
      "source": [
        "Our objective is to discover a linear function model (equation) that can effectively capture the relationship between the $x$ and $y$ dataset.\n",
        "\n",
        "In fact, instead of employing PyTorch, we can use `np.polyfit()` to fit the dataset. This allows us to acquire the fitting parameters $w$ and $b$ for the linear function $y = wx + b$, where $w$ and $b$ are referred to as the weight and bias, respectively.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REAk1xWzG4cs"
      },
      "source": [
        "w, b = np.polyfit(x, y, 1)\n",
        "print(f\"w={w:.3f}, b={b:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beBJ2jmvOnsi"
      },
      "source": [
        "Due to the presence of noise in the $x-y$ dataset, the obtained values of $w$ and $b$ may not match the exact values used to create the dataset. Nevertheless, they are typically close enough to allow us to create a fitting line using these parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIJCGZ_5KD8"
      },
      "source": [
        "y_fit = w*x + b\n",
        "\n",
        "plt.plot(x, y, marker=\"o\", lw=0, label=\"dataset\")\n",
        "plt.plot(x, y_fit, lw=2, label=\"fit\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAT9dny8FhRW"
      },
      "source": [
        "While `np.polyfit()` works effectively, in this example, we will achieve the same outcome using PyTorch.\n",
        "\n",
        "Let's start by converting the $x$ and $y$ dataset into `torch.tensor` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mbf0APf5yYF"
      },
      "source": [
        "x = torch.tensor(x)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "print(type(x))\n",
        "print(type(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWY-KZbAJopr"
      },
      "source": [
        "Next, we define a function called `model()` that predicts the output value `y` based on an input value `x` using the parameters `w` and `b`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-FhluBo7Ea1"
      },
      "source": [
        "def model(x):\n",
        "    return w*x + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vemQxk48K_mz"
      },
      "source": [
        "We also define a function called `loss_func()` to calculate the mean squared error between `p` and `y`, where `p` represents the output values (predictions) of the `model()`. This type of function is referred to as a loss function, which helps measure the degree of error in the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do3Ll8sEkMb6"
      },
      "source": [
        "$\\displaystyle \\mathrm{loss} = \\mathrm{mse}(p, y) = \\frac{1}{N}\\sum_{i=0}^{N-1}(p_i-y_i)^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJX-xDCl7Le1"
      },
      "source": [
        "def loss_func(p, y):\n",
        "    return ((p-y)**2).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZdLCxF8NkXD"
      },
      "source": [
        "At this stage, we haven't determined the values of `w` and `b` yet. Therefore, we initialize these values with arbitrary numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_fsb7vWO6Dy"
      },
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)   # you can set any number here\n",
        "b = torch.tensor(-5.0, requires_grad=True)  # you can set any number here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_4S2EQYO8wl"
      },
      "source": [
        "Now, we can make a prediction using `model()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLHoWRI7O8Rz"
      },
      "source": [
        "p = model(x)\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtXwQ3nIttbt"
      },
      "source": [
        "Let's visualize the current prediction. Since we've initialized `w` and `b` with arbitrary numbers, it's expected that the model's prediction won't fit the data well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gvw_X_MPKhL"
      },
      "source": [
        "plt.plot(x, y, marker=\"o\", lw=0, label=\"data\")\n",
        "plt.plot(x, p.detach().numpy(), label=\"prediction\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV7CxsLjPsnR"
      },
      "source": [
        "Next, we calculate the loss value (mean squared error) using `loss_func()`. It's important to note that the loss value will be very large because `w` and `b` are arbitrary values and have not been optimized yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIXfD63wPzVm"
      },
      "source": [
        "loss = loss_func(p, y)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roLiOg-dQp_q"
      },
      "source": [
        "To optimize `w` and `b`, we must determine the gradients of the loss with respect to the current values of `w` and `b`. This can be achieved using `loss.backward()`. The gradients, denoted as $\\displaystyle \\frac{\\partial \\mathrm{loss}}{\\partial w}$ and $\\displaystyle \\frac{\\partial \\mathrm{loss}}{\\partial b}$, can be accessed via `w.grad` and `b.grad`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNT7UuCnQh2X"
      },
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWPnNfKsTLW_"
      },
      "source": [
        "We can update `w` and `b` using the following equations, where $\\eta$ represents the learning rate. This method is known as stochastic gradient descent (SGD):\n",
        "\n",
        "$\\displaystyle w := w - \\eta\\frac{\\partial\\mathrm{loss}}{\\partial w}$\n",
        "\n",
        "$\\displaystyle b := b - \\eta\\frac{\\partial\\mathrm{loss}}{\\partial b}$\n",
        "\n",
        "When updating these values, we don't want to compute gradients. To achieve this, we use with `torch.no_grad()` at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpoxdQ0WTK9X"
      },
      "source": [
        "lr = 0.01    # define learning rate\n",
        "\n",
        "with torch.no_grad():    # disable gradients calculations\n",
        "    w -= w.grad*lr       # update w\n",
        "    b -= b.grad*lr       # update b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aen0JWycU6BN"
      },
      "source": [
        "At this stage, you'll notice that the values of `w` and `b` are closer to their true values ($w$=5.0, $b$=3.0) compared to the initial values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H6yLMDgU4Yr"
      },
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G7G6xIpYnlk"
      },
      "source": [
        "To further optimize `w` and `b`, we repeat the above process multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVJCrKHR7UPk"
      },
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)   # you can set any number here\n",
        "b = torch.tensor(-5.0, requires_grad=True)  # you can set any number here\n",
        "\n",
        "lr = 0.01    # learning rate\n",
        "epochs = 100  # how many times we repeat training\n",
        "\n",
        "w = torch.tensor(3.0, requires_grad=True)    # initialize w\n",
        "b = torch.tensor(-1.0, requires_grad=True)   # initialize b\n",
        "\n",
        "# empty lists for saving loss, w, b\n",
        "loss_list = []\n",
        "w_list = []\n",
        "b_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    p = model(x)              # prediction\n",
        "    loss = loss_func(p, y)    # measure loss\n",
        "    loss.backward()           # determine gradients\n",
        "\n",
        "    with torch.no_grad():     # disable autograd\n",
        "        w -= w.grad*lr        # update w\n",
        "        b -= b.grad*lr        # update b\n",
        "\n",
        "        w.grad.zero_() # reset gradient\n",
        "        b.grad.zero_() # reset gradient\n",
        "\n",
        "    # save loss, w, b\n",
        "    loss_list.append(loss.item())\n",
        "    w_list.append(w.item())\n",
        "    b_list.append(b.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, loss={loss.item():.3f}, w={w.item():.3f}, b={b.item():.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7wmY5X4VF8I"
      },
      "source": [
        "As you can see now, `w` is now close to 5.0, and `b` is close to 3.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prLVOe3NUHXZ"
      },
      "source": [
        "Let's visualize how the loss value has decreased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKNMYl8W-YPu"
      },
      "source": [
        "plt.plot(np.arange(epochs)+1, loss_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuWi5tIEUNty"
      },
      "source": [
        "Let's visualize how the `w` value has changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g9-IoVMT5dB"
      },
      "source": [
        "plt.plot(np.arange(epochs)+1, w_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"w\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NimP-NFUW7p"
      },
      "source": [
        "Let's visualize how the `b` value has changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hgnOPXWUA_p"
      },
      "source": [
        "plt.plot(np.arange(epochs)+1, b_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"b\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the optimized values of `w` and `b`, the model fits the dataset very well."
      ],
      "metadata": {
        "id": "ZbWEnmyEEwEE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYXitcRHQQ9L"
      },
      "source": [
        "plt.plot(x, y, marker=\"o\", lw=0, label=\"dataset\")\n",
        "plt.plot(x, p.detach().numpy(), label=\"prediction\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU83IdbgUqfo"
      },
      "source": [
        "Now you can try to change initial values of `epochs`, `lr`, `w`, `b` etc., and let's observe the results."
      ]
    }
  ]
}